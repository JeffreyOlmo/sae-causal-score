{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"4,5,6,7\"\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "torch.set_grad_enabled(False);\n",
    "# package import\n",
    "from torch import Tensor\n",
    "from transformer_lens import utils\n",
    "from functools import partial\n",
    "from jaxtyping import Int, Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "# Choose a layer you want to focus on\n",
    "# For this tutorial, we're going to use layer 2\n",
    "layer = 8\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\").to('cuda')\n",
    "\n",
    "# Initialize SAE\n",
    "layer = 8\n",
    "sae, cfg_dict, _ = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=f\"blocks.{layer}.hook_resid_pre\",\n",
    "    device= \"cuda:0\"\n",
    ")\n",
    "\n",
    "# get hook point\n",
    "hook_point = sae.cfg.hook_name\n",
    "print(hook_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_active_directions_encoder(sae: SAE, activation: torch.Tensor, k: int) -> torch.Tensor:\n",
    "    activation = activation.to(sae.W_enc.device)\n",
    "    latent = sae.encode(activation)\n",
    "    _, top_k_indices = torch.topk(latent.abs().squeeze(), k)\n",
    "    top_k_directions = sae.W_enc.T[top_k_indices].T\n",
    "    unit_vectors = torch.nn.functional.normalize(top_k_directions, p=2, dim=1)\n",
    "    return unit_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_active_directions_decoder(sae: SAE, activation: torch.Tensor, k: int) -> torch.Tensor:\n",
    "    activation = activation.to(sae.W_dec.device)\n",
    "    latent = sae.encode(activation)\n",
    "    _, top_k_indices = torch.topk(latent.abs().squeeze(), k)\n",
    "    top_k_directions = sae.W_dec[top_k_indices].T\n",
    "    unit_vectors = torch.nn.functional.normalize(top_k_directions, p=2, dim=1)\n",
    "    return unit_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "\n",
    "def get_last_token_directions(model: HookedTransformer, sae: SAE, prompt: str, k: int = 5) -> torch.Tensor:\n",
    "    # Tokenize the prompt\n",
    "    tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "    \n",
    "    # Run the model and get the cache\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    \n",
    "    # Get the activation for the last token\n",
    "    last_token_activation = cache[sae.cfg.hook_name][:, -1, :]\n",
    "    \n",
    "    # Get the top k active directions\n",
    "    top_k_directions = get_top_k_active_directions(sae, last_token_activation, k)\n",
    "    \n",
    "    return top_k_directions\n",
    "\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "k = 5\n",
    "\n",
    "try:\n",
    "    top_directions = get_last_token_directions(model, sae, prompt, k)\n",
    "    print(\"Successfully computed top directions\")\n",
    "    print(f\"Top directions shape: {top_directions.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# If successful, compute angles\n",
    "if 'top_directions' in locals():\n",
    "    angles = torch.zeros((k, k))\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            cos_similarity = torch.dot(top_directions[i], top_directions[j])\n",
    "            angle = torch.acos(torch.clamp(cos_similarity, -1, 1)) * 180 / torch.pi\n",
    "            angles[i, j] = angle\n",
    "\n",
    "    print(\"\\nAngles between directions (in degrees):\")\n",
    "    print(angles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from typing import NamedTuple, Literal, Callable\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class CausalBasis(NamedTuple):\n",
    "    energies: th.Tensor\n",
    "    vectors: th.Tensor\n",
    "\n",
    "def compute_direction_mean(\n",
    "    batch: dict,\n",
    "    model: th.nn.Module,\n",
    "    layer_index: int,\n",
    "    direction: th.Tensor\n",
    ") -> float:\n",
    "    total_projection = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with th.no_grad():\n",
    "        input_ids = batch[\"input_ids\"].to(direction.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(direction.device)\n",
    "\n",
    "        _, cache = model.run_with_cache(input_ids, attention_mask=attention_mask)\n",
    "        activations = cache[f\"blocks.{layer_index}.hook_resid_pre\"]\n",
    "\n",
    "        # Project activations onto the direction\n",
    "        projection = th.einsum(\"bld,d->bl\", activations, direction)\n",
    "        \n",
    "        # Apply attention mask and sum\n",
    "        masked_projection = projection * attention_mask\n",
    "        batch_sum = masked_projection.sum()\n",
    "        batch_tokens = attention_mask.sum()\n",
    "\n",
    "        total_projection += batch_sum.item()\n",
    "        total_tokens += batch_tokens.item()\n",
    "\n",
    "    return total_projection / total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "def remove_subspace(u: th.Tensor, v: th.Tensor, direction_mean: th.Tensor, mode: str = \"mean\") -> th.Tensor:\n",
    "    v = v.squeeze()  # Ensure v is 1D\n",
    "    v_normalized = v / v.norm()\n",
    "    if mode == \"mean\":\n",
    "        proj_u = th.einsum(\"bi,i->b\", u, v_normalized).unsqueeze(1) * v_normalized.unsqueeze(0)\n",
    "        result = u - proj_u + direction_mean\n",
    "        \n",
    "    elif mode == \"zero\":\n",
    "        result = u - th.einsum(\"bi,i->b\", u, v_normalized).unsqueeze(1) * v_normalized.unsqueeze(0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode {mode}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import matplotlib.pyplot as plt\n",
    "import einops\n",
    "\n",
    "def compute_functional_similarity(orig_activations, abl_activations, orig_logits, abl_logits, beta=0):\n",
    "    # Compute mean squared difference for each layer's activations\n",
    "    msd_list = []\n",
    "    for orig_act, abl_act in zip(orig_activations[:-1], abl_activations[:-1]):  # Exclude logits\n",
    "        msd = th.mean((th.norm(orig_act) - th.norm(abl_act))**2)\n",
    "        msd_list.append(msd)\n",
    "    \n",
    "    # Average mean squared difference across layers\n",
    "    if msd_list:\n",
    "        avg_msd = th.mean(th.stack(msd_list))\n",
    "    else:\n",
    "        # If there are no intermediate activations, use only the logits\n",
    "        avg_msd = th.mean((orig_logits - abl_logits) ** 2)\n",
    "\n",
    "    orig_logits_flat = einops.rearrange(orig_logits, \"... vocab -> (...) vocab\")\n",
    "    abl_logits_flat = einops.rearrange(abl_logits, \"... vocab -> (...) vocab\")\n",
    "\n",
    "    \n",
    "    # Compute KL divergence for logits\n",
    "    kl_div = F.kl_div(\n",
    "        F.log_softmax(abl_logits_flat, dim=-1),\n",
    "        F.log_softmax(orig_logits_flat, dim=-1),\n",
    "        log_target=True,\n",
    "        reduction=\"batchmean\",\n",
    "    )\n",
    "    # Combine average MSD and KL divergence\n",
    "    # Note: We use negative MSD because we want to maximize similarity (minimize difference)\n",
    "    similarity = beta * avg_msd + (1 - beta) * kl_div\n",
    "    \n",
    "    return similarity, avg_msd, kl_div\n",
    "\n",
    "def compute_functional_similarity_logits_only(orig_logits, abl_logits, beta=0):\n",
    "    orig_logits_flat = einops.rearrange(orig_logits, \"... vocab -> (...) vocab\")\n",
    "    abl_logits_flat = einops.rearrange(abl_logits, \"... vocab -> (...) vocab\")\n",
    "\n",
    "    # Compute KL divergence for logits\n",
    "    kl_div = F.kl_div(\n",
    "        F.log_softmax(abl_logits_flat, dim=-1),\n",
    "        F.log_softmax(orig_logits_flat, dim=-1),\n",
    "        log_target=True,\n",
    "        reduction=\"batchmean\",\n",
    "    )\n",
    "    similarity = kl_div\n",
    "    \n",
    "    return similarity, 0, kl_div\n",
    "\n",
    "def compute_subsequent_outputs(model: th.nn.Module, activation: th.Tensor, layer_index: int) -> list[th.Tensor]:\n",
    "    outputs = [activation]  # Include the initial activation\n",
    "\n",
    "    if activation.ndim == 2:\n",
    "        activation = activation.unsqueeze(1)  # Add pos dimension if missing\n",
    "\n",
    "    current_activation = activation\n",
    "\n",
    "    for block in model.blocks[layer_index+1:]:\n",
    "        # Pass through the complete block\n",
    "        current_activation = block(current_activation)\n",
    "        outputs.append(current_activation)\n",
    "\n",
    "    # Apply the final layer normalization\n",
    "    current_activation = model.ln_final(current_activation)\n",
    "\n",
    "    # Compute the logits\n",
    "    logits = model.unembed(current_activation)\n",
    "    outputs.append(logits)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def compute_subsequent_outputs_logits_only(model: th.nn.Module, activation: th.Tensor, layer_index: int) -> list[th.Tensor]:\n",
    "    if activation.ndim == 2:\n",
    "        activation = activation.unsqueeze(1) \n",
    "    current_activation = activation\n",
    "\n",
    "    for block in model.blocks[layer_index+1:]:\n",
    "        current_activation = block(current_activation)\n",
    "    current_activation = model.ln_final(current_activation)\n",
    "    logits = model.unembed(current_activation)\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "\n",
    "def ablation_operation(v: torch.Tensor, u: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implement the ablation operation v âŠ– u.\n",
    "    \"\"\"\n",
    "    return torch.max(torch.zeros_like(v), v - torch.abs(u))\n",
    "\n",
    "def find_ablation_magnitude(sae: SAE, activation: torch.Tensor, latent_direction: torch.Tensor, threshold: float = 0.5) -> float:\n",
    "    \"\"\"\n",
    "    Find the magnitude c such that the latent just stops activating.\n",
    "    \"\"\"\n",
    "    c = 0.0\n",
    "    step = 1.0\n",
    "    max_iterations = 100\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        ablated_activation = ablation_operation(activation, c * latent_direction)\n",
    "        latent = sae.encode(ablated_activation)\n",
    "        if latent[latent_direction] < threshold:\n",
    "            return c\n",
    "        c += step\n",
    "        \n",
    "    return c  # Return the last c if we didn't converge\n",
    "\n",
    "def compute_sq_score(sae: SAE, model: torch.nn.Module, activations: torch.Tensor, latent_index: int, layer_index: int, num_samples: int = 10):\n",
    "    \"\"\"\n",
    "    Compute the SQ score for a given latent.\n",
    "    \"\"\"\n",
    "    device = activations.device\n",
    "    latent_direction = sae.W_dec[latent_index].T.to(device)\n",
    "    \n",
    "    sq_scores = []\n",
    "    \n",
    "    for activation in activations:\n",
    "        # Find c such that the latent just stops activating\n",
    "        c = find_ablation_magnitude(sae, activation, latent_direction)\n",
    "        \n",
    "        # Compute x-\n",
    "        x_minus = ablation_operation(activation, c * latent_direction)\n",
    "        \n",
    "        # Compute functional difference\n",
    "        orig_logits = compute_subsequent_outputs_logits_only(model, activation.unsqueeze(0), layer_index)\n",
    "        abl_logits = compute_subsequent_outputs_logits_only(model, x_minus.unsqueeze(0), layer_index)\n",
    "        \n",
    "        s_l, _, _ = compute_functional_similarity_logits_only(orig_logits, abl_logits)\n",
    "        \n",
    "        # Compute S_max\n",
    "        s_max_samples = []\n",
    "        for _ in range(num_samples):\n",
    "            random_direction = torch.randn_like(latent_direction)\n",
    "            random_direction = random_direction / random_direction.norm() * c\n",
    "            x_random = ablation_operation(activation, random_direction)\n",
    "            random_logits = compute_subsequent_outputs_logits_only(model, x_random.unsqueeze(0), layer_index)\n",
    "            s_random, _, _ = compute_functional_similarity_logits_only(orig_logits, random_logits)\n",
    "            s_max_samples.append(s_random)\n",
    "        \n",
    "        s_max = max(s_max_samples)\n",
    "        \n",
    "        # Compute SQ score for this activation\n",
    "        sq_score = s_l / s_max if s_max > 0 else 0\n",
    "        sq_scores.append(sq_score)\n",
    "    \n",
    "    # Return the average SQ score\n",
    "    return torch.mean(torch.stack(sq_scores))\n",
    "\n",
    "# Example usage\n",
    "sae = SAE(...)  # Your SAE model\n",
    "model = TransformerModel(...)  # Your main model\n",
    "activations = ...  # Your set of activations that activate the latent\n",
    "latent_index = ...  # The index of the latent you want to evaluate\n",
    "layer_index = ...  # The index of the layer where the activations are from\n",
    "\n",
    "sq_score = compute_sq_score(sae, model, activations, latent_index, layer_index)\n",
    "print(f\"SQ Score for latent {latent_index}: {sq_score.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
