
Finetuned SAE logged to wandb as artifact: finetuned_sae_step_0
Run name: 24576-L1-0.005-LR-1e-06-Tokens-1.000e+07
n_tokens_per_buffer (millions): 0.16384
Lower bound: n_contexts_per_buffer (millions): 0.00064
Total training steps: 25000
Total wandb updates: 2500
n_tokens_per_feature_sampling_window (millions): 204.8
n_tokens_per_dead_feature_window (millions): 102.4
We will reset the sparsity calculation 12 times.
Number tokens in sparsity calculation window: 8.00e+05
Loaded pretrained model gpt2-small into HookedTransformer
Objective value: 7284788.0000:   2%|▏         | 2/100 [00:00<00:00, 183.56it/s]
/usr/local/lib/python3.10/dist-packages/sae_lens/training/training_sae.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  out = torch.tensor(origin, dtype=self.dtype, device=self.device)
mse and l0 loss: 80079.9375
Run name: 24576-L1-0.005-LR-1e-06-Tokens-1.000e+07
n_tokens_per_buffer (millions): 0.16384
Lower bound: n_contexts_per_buffer (millions): 0.00064
Total training steps: 25000
Total wandb updates: 2500
n_tokens_per_feature_sampling_window (millions): 204.8
n_tokens_per_dead_feature_window (millions): 102.4
We will reset the sparsity calculation 12 times.
Number tokens in sparsity calculation window: 8.00e+05
Loaded pretrained model gpt2-small into HookedTransformer
Objective value: 7284788.0000:   2%|▏         | 2/100 [00:00<00:00, 112.45it/s]
mse and l0 loss: 85403.2421875
R_loss: 0.9209823564306134
Average R_score: 0.07901764356938656
Run name: 24576-L1-0.005-LR-1e-06-Tokens-1.000e+07
n_tokens_per_buffer (millions): 0.16384
Lower bound: n_contexts_per_buffer (millions): 0.00064
Total training steps: 25000
Total wandb updates: 2500
n_tokens_per_feature_sampling_window (millions): 204.8
n_tokens_per_dead_feature_window (millions): 102.4
We will reset the sparsity calculation 12 times.
Number tokens in sparsity calculation window: 8.00e+05
Loaded pretrained model gpt2-small into HookedTransformer
Objective value: 7284788.0000:   2%|▏         | 2/100 [00:00<00:00, 181.56it/s]
/usr/local/lib/python3.10/dist-packages/sae_lens/training/training_sae.py:455: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  out = torch.tensor(origin, dtype=self.dtype, device=self.device)
0.18571871519088745
0.04790434241294861
0.09641484171152115
0.12504421174526215
0.09995363652706146
0.0856088250875473
0.08823305368423462
0.09152861684560776
0.10048480331897736
0.09710900485515594
0.04432161524891853
0.063373863697052
0.05516344681382179
0.08853589743375778
0.06088969483971596
0.05698619410395622
0.058105114847421646
0.06454935669898987
0.07796324789524078
0.05070607364177704
0.10311684012413025
0.12932583689689636
0.04713510721921921
0.06555266678333282
0.06821535527706146
0.07425470650196075
0.058671873062849045
0.07070693373680115
0.10495506972074509
0.10950169712305069
0.10896111279726028
0.047712069004774094
0.10591582208871841
0.04069232568144798
0.08595804870128632
0.07940943539142609
0.05420179292559624
0.10004792362451553
0.05968441441655159
0.0450785756111145
0.04092429578304291
0.047544803470373154
0.11647336184978485
0.05336159095168114
0.098068967461586
0.03805026784539223
0.0852084532380104
0.059550635516643524
0.06930068135261536
0.05745433643460274
0.07684963941574097
0.08573649823665619
0.10265790671110153
0.08715895563364029
0.05325164645910263
0.07028725743293762
0.06032348424196243
0.07053781300783157
0.0412060022354126
0.04454903304576874
0.13454419374465942
0.0552317351102829
0.07735025882720947
0.08267363160848618
0.02796046808362007
0.07525645941495895
0.051693692803382874
0.04890215024352074
0.04111531004309654
0.07897994667291641
0.09072350710630417
0.05729806050658226
0.0718737244606018
0.09373701363801956
0.11805825680494308
0.060196422040462494
0.08165380358695984
0.09097832441329956
0.07086686044931412
0.06291048973798752
0.10532701015472412
0.05585220828652382
0.07547879964113235
0.0690392479300499
0.11032598465681076
0.13222761452198029
0.08975018560886383
0.06954002380371094
0.08473867177963257
0.07298414409160614
0.06882473081350327
0.05578183755278587
0.07111615687608719
0.13532929122447968
0.13818359375
0.12994074821472168
0.07376684993505478
0.10266229510307312
0.12806211411952972
0.10154207795858383
0.03986092284321785
0.0506756529211998
0.06468204408884048
0.12720155715942383
0.05118550732731819
0.08925436437129974
0.13444273173809052
0.05657631531357765
0.029963990673422813
0.09012878686189651
0.07437224686145782
0.12202387303113937
0.08607388287782669
0.05179813504219055
0.04819822311401367
0.12374763190746307
0.11922851949930191
0.09682593494653702
0.046697668731212616
0.10800006240606308
0.04334626346826553
0.046401746571063995
0.11488784104585648
0.03402430936694145
0.07462739199399948
0.06874058395624161
0.08031836152076721
0.05228467285633087
0.04032432660460472
0.04359615594148636
0.07036510109901428
0.04727786406874657
0.06539962440729141
0.11367922276258469
0.07269241660833359
0.0641302540898323
0.09667433798313141
0.10722511261701584
0.04401978850364685
0.08837291598320007
0.08991377055644989
0.11394107341766357
0.09394306689500809
0.11748407781124115
0.09362014383077621
0.07851532101631165
0.11285579949617386
0.10245900601148605
0.06255783140659332
0.044758059084415436
0.07283812016248703
0.11768261343240738
0.061463963240385056
0.07519882917404175
0.08321323245763779
0.10540710389614105
0.0834331139922142
0.05799232795834541
0.05473073571920395
0.08675730228424072
0.09484871476888657
0.055929407477378845
0.035863734781742096
0.06816431879997253
0.05442460998892784
0.10402917116880417
0.0788409486413002
0.04202837124466896
0.10567555576562881
0.09168317914009094
0.08387009799480438
0.04871902987360954
0.09330764412879944
0.045607324689626694
0.06276222318410873
0.0715864971280098
0.1170153021812439
0.05248824134469032
0.03914185240864754
0.05044064670801163
0.057881299406290054
0.0875217616558075
0.10290630161762238
0.056113723665475845
0.10337889939546585
0.06957811862230301
0.059409063309431076
0.07991553843021393
0.0861058458685875
0.08041060715913773
0.10457702726125717
0.1187622919678688
0.06203393265604973
0.10937163233757019
0.12767474353313446
0.1261429339647293
0.05754396319389343
0.06581459194421768
0.08736108243465424
0.043938636779785156
0.10088160634040833
0.07738226652145386
0.09176628291606903
0.054565757513046265
0.05859926715493202
0.0314209945499897
0.06584790349006653
0.04202369973063469
0.0769011452794075
0.06559334695339203
0.06444645673036575
0.09295788407325745
0.041105836629867554
0.04514816030859947
0.07211850583553314
0.08793468773365021
0.06472219526767731
0.1050412580370903
0.08381780982017517
0.04368634894490242
0.07336856424808502
0.0804598480463028
0.0432480052113533
0.07148434221744537
0.07115396857261658
0.12358193844556808
0.05473585054278374
0.05023343488574028
0.114179328083992
0.07735875248908997
0.07749949395656586
0.0921473428606987
0.024476706981658936
0.038281071931123734
0.06256778538227081
0.08938293159008026
0.11729928851127625
0.017386209219694138
0.09650912880897522
0.12475118041038513
0.0890892818570137
0.060801099985837936
0.03705869987607002
0.0911843553185463
0.07857783138751984
0.05744592472910881
0.09396278113126755
0.09043203294277191
0.12165192514657974
0.09180574864149094
0.1276201456785202
0.10916251689195633
0.08595563471317291
0.09204910695552826
0.06367582082748413
0.1386452466249466
mse and l0 loss: 88699.234375
R_loss: 0.9220124565254082
Average R_score: 0.0779875434745918
Run name: 24576-L1-0.005-LR-1e-06-Tokens-1.000e+07
n_tokens_per_buffer (millions): 0.16384
Lower bound: n_contexts_per_buffer (millions): 0.00064
Total training steps: 25000
Total wandb updates: 2500
n_tokens_per_feature_sampling_window (millions): 204.8
n_tokens_per_dead_feature_window (millions): 102.4
We will reset the sparsity calculation 12 times.
Number tokens in sparsity calculation window: 8.00e+05
Loaded pretrained model gpt2-small into HookedTransformer
Objective value: 7284788.0000:   3%|▎         | 3/100 [00:00<00:00, 206.84it/s]
mse and l0 loss: 83474.875
R_loss: 0.9228464961051941
Average R_score: 0.07715349644422531
gradient_norm: 0.28600531816482544
mse and l0 loss: 82611.875
R_loss: 0.9215946793556213
Average R_score: 0.07840531319379807
gradient_norm: 0.25871172547340393
mse and l0 loss: 81631.2421875
R_loss: 0.921975314617157
Average R_score: 0.07802469283342361
gradient_norm: 0.26667648553848267
mse and l0 loss: 83361.3359375
R_loss: 0.9206466674804688
Average R_score: 0.07935334742069244
gradient_norm: 0.3415902554988861
mse and l0 loss: 81187.3671875
R_loss: 0.9224354028701782
Average R_score: 0.07756459712982178
gradient_norm: 0.2653522789478302
mse and l0 loss: 81969.0078125
R_loss: 0.9230133891105652
Average R_score: 0.07698662579059601
gradient_norm: 0.2617167830467224
mse and l0 loss: 82375.3984375
R_loss: 0.9209882020950317
Average R_score: 0.07901177555322647
gradient_norm: 0.2618400454521179
mse and l0 loss: 80725.7890625
R_loss: 0.9218769669532776
Average R_score: 0.07812301069498062
gradient_norm: 0.25778037309646606
mse and l0 loss: 82710.84375
R_loss: 0.9254832863807678
Average R_score: 0.07451670616865158
gradient_norm: 0.28879883885383606
mse and l0 loss: 81555.640625
R_loss: 0.9241019487380981
Average R_score: 0.07589808106422424
gradient_norm: 0.2604953348636627
Training interrupted. Saving checkpoint...
Run name: 24576-L1-0.005-LR-1e-07-Tokens-1.000e+07
n_tokens_per_buffer (millions): 0.16384
Lower bound: n_contexts_per_buffer (millions): 0.00064
Total training steps: 25000
Total wandb updates: 2500
n_tokens_per_feature_sampling_window (millions): 204.8
n_tokens_per_dead_feature_window (millions): 102.4
We will reset the sparsity calculation 12 times.
Number tokens in sparsity calculation window: 8.00e+05
Loaded pretrained model gpt2-small into HookedTransformer
Objective value: 7284788.5000:   3%|▎         | 3/100 [00:00<00:00, 205.44it/s]
Finetuned SAE saved locally to: ./checkpoints/finetuned_sae_step_0
Finetuned SAE logged to wandb as artifact: finetuned_sae_step_0
mse and l0 loss: 82917.9609375
R_loss: 0.922728955745697
Average R_score: 0.07727102190256119
gradient_norm: 0.288771390914917
[34m[1mwandb[39m[22m: [33mWARNING[39m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 9. Dropping entry: {'gradient_norm': 0.288771390914917, '_timestamp': 1721140136.8524394}).
mse and l0 loss: 82312.0625
R_loss: 0.9215125441551208
Average R_score: 0.07848744839429855
gradient_norm: 0.26417267322540283
mse and l0 loss: 81170.8046875
R_loss: 0.9228237271308899
Average R_score: 0.07717625796794891
gradient_norm: 0.2650827169418335
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 9. Dropping entry: {'gradient_norm': 0.26417267322540283, '_timestamp': 1721140143.4759967}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 9. Dropping entry: {'gradient_norm': 0.2650827169418335, '_timestamp': 1721140150.10408}).
mse and l0 loss: 82888.015625
R_loss: 0.9216544032096863
Average R_score: 0.07834558933973312
gradient_norm: 0.3180754780769348
mse and l0 loss: 80952.7265625
R_loss: 0.9221512079238892
Average R_score: 0.07784877717494965
gradient_norm: 0.27516046166419983
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 9. Dropping entry: {'gradient_norm': 0.3180754780769348, '_timestamp': 1721140156.7965543}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 9. Dropping entry: {'gradient_norm': 0.27516046166419983, '_timestamp': 1721140163.5550194}).
mse and l0 loss: 81473.765625
R_loss: 0.9220185875892639
Average R_score: 0.07798143476247787
gradient_norm: 0.2604491412639618
mse and l0 loss: 82251.59375
R_loss: 0.924620509147644
Average R_score: 0.07537952065467834
gradient_norm: 0.2624770700931549
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 9. Dropping entry: {'gradient_norm': 0.2604491412639618, '_timestamp': 1721140170.3772528}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 9. Dropping entry: {'gradient_norm': 0.2624770700931549, '_timestamp': 1721140177.053413}).
mse and l0 loss: 80371.734375
R_loss: 0.9248174428939819
Average R_score: 0.07518254965543747
gradient_norm: 0.2620503604412079
mse and l0 loss: 82287.84375
R_loss: 0.9226190447807312
Average R_score: 0.0773809552192688
gradient_norm: 0.28648361563682556
mse and l0 loss: 81019.6015625
R_loss: 0.9236850142478943
Average R_score: 0.07631497830152512
gradient_norm: 0.26222774386405945
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 7 is less than current step: 9. Dropping entry: {'gradient_norm': 0.2620503604412079, '_timestamp': 1721140183.7533145}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 8 is less than current step: 9. Dropping entry: {'gradient_norm': 0.28648361563682556, '_timestamp': 1721140190.4422472}).
mse and l0 loss: 81011.1640625
R_loss: 0.9208075404167175
Average R_score: 0.07919247448444366
gradient_norm: 0.25639623403549194
mse and l0 loss: 82133.765625
R_loss: 0.9233658313751221
Average R_score: 0.07663414627313614
gradient_norm: 0.2857934534549713
mse and l0 loss: 81436.1328125
R_loss: 0.9242978692054749
Average R_score: 0.07570213079452515
gradient_norm: 0.2654670774936676
mse and l0 loss: 81564.546875
R_loss: 0.9248794317245483
Average R_score: 0.07512059807777405
gradient_norm: 0.2551979720592499
mse and l0 loss: 81222.515625
R_loss: 0.9214121103286743
Average R_score: 0.07858786731958389
gradient_norm: 0.2572227716445923
mse and l0 loss: 82893.1875
R_loss: 0.924829363822937
Average R_score: 0.0751706063747406
gradient_norm: 0.2635273039340973
mse and l0 loss: 81474.1796875
R_loss: 0.9212378263473511
Average R_score: 0.07876219600439072
gradient_norm: 0.2568233609199524
mse and l0 loss: 81161.96875
R_loss: 0.9238988757133484
Average R_score: 0.07610113173723221
gradient_norm: 0.2610567510128021
mse and l0 loss: 82550.78125
R_loss: 0.9226961135864258
Average R_score: 0.07730385661125183
gradient_norm: 0.25355860590934753
mse and l0 loss: 77606.7109375
R_loss: 0.9220455288887024
Average R_score: 0.07795447111129761
gradient_norm: 0.2641110420227051
mse and l0 loss: 83619.2890625
R_loss: 0.9263715147972107
Average R_score: 0.07362846285104752
gradient_norm: 0.27167925238609314
mse and l0 loss: 81336.4609375
R_loss: 0.9266177415847778
Average R_score: 0.07338227331638336
gradient_norm: 0.2632383108139038
mse and l0 loss: 81281.3046875
R_loss: 0.9243589639663696
Average R_score: 0.07564103603363037
gradient_norm: 0.25878989696502686
mse and l0 loss: 82283.6796875
R_loss: 0.9265097379684448
Average R_score: 0.07349027693271637
gradient_norm: 0.26219046115875244
mse and l0 loss: 81145.828125
R_loss: 0.9237703084945679
Average R_score: 0.07622969895601273
gradient_norm: 0.2876933515071869
mse and l0 loss: 81680.03125
R_loss: 0.923570990562439
Average R_score: 0.07642899453639984
gradient_norm: 0.26889121532440186
mse and l0 loss: 82092.3046875
R_loss: 0.9208515286445618
Average R_score: 0.07914848625659943
gradient_norm: 0.26219385862350464
mse and l0 loss: 82584.2421875
R_loss: 0.9216452836990356
Average R_score: 0.07835472375154495
gradient_norm: 0.25928619503974915
mse and l0 loss: 82052.8203125
R_loss: 0.9229583740234375
Average R_score: 0.0770416259765625
gradient_norm: 0.2695407569408417
mse and l0 loss: 81875.9140625
R_loss: 0.9228499531745911
Average R_score: 0.07715006917715073
gradient_norm: 0.26426705718040466
mse and l0 loss: 82438.078125
R_loss: 0.9236154556274414
Average R_score: 0.0763845443725586
gradient_norm: 0.26068615913391113
mse and l0 loss: 82463.3046875
R_loss: 0.92389315366745
Average R_score: 0.07610683143138885
gradient_norm: 0.26003795862197876
Training interrupted. Saving checkpoint...
Run name: 24576-L1-0.005-LR-0.0001-Tokens-1.000e+07
n_tokens_per_buffer (millions): 0.16384
Lower bound: n_contexts_per_buffer (millions): 0.00064
Total training steps: 25000
Total wandb updates: 2500
n_tokens_per_feature_sampling_window (millions): 204.8
n_tokens_per_dead_feature_window (millions): 102.4
We will reset the sparsity calculation 12 times.
Number tokens in sparsity calculation window: 8.00e+05
Loaded pretrained model gpt2-small into HookedTransformer
Objective value: 7284788.0000:   3%|▎         | 3/100 [00:00<00:00, 199.98it/s]
mse and l0 loss: 85527.3046875
R_loss: 0.918744683265686
Average R_score: 0.08125530928373337
gradient_norm: 0.31371763348579407
mse and l0 loss: 84733.515625
R_loss: 0.9246432781219482
Average R_score: 0.07535673677921295
gradient_norm: 0.26539215445518494
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 0 is less than current step: 31. Dropping entry: {'gradient_norm': 0.31371763348579407, '_timestamp': 1721140501.7525294}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 1 is less than current step: 31. Dropping entry: {'gradient_norm': 0.26539215445518494, '_timestamp': 1721140508.4959893}).
mse and l0 loss: 83536.90625
R_loss: 0.9237943291664124
Average R_score: 0.07620569318532944
gradient_norm: 0.2641485035419464
mse and l0 loss: 85651.484375
R_loss: 0.9210596084594727
Average R_score: 0.07894039154052734
gradient_norm: 0.3601401746273041
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 2 is less than current step: 31. Dropping entry: {'gradient_norm': 0.2641485035419464, '_timestamp': 1721140515.3105018}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 3 is less than current step: 31. Dropping entry: {'gradient_norm': 0.3601401746273041, '_timestamp': 1721140521.994676}).
mse and l0 loss: 83333.15625
R_loss: 0.9237489104270935
Average R_score: 0.07625110447406769
gradient_norm: 0.26535913348197937
mse and l0 loss: 83891.6875
R_loss: 0.9229825139045715
Average R_score: 0.07701746374368668
gradient_norm: 0.25806814432144165
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 4 is less than current step: 31. Dropping entry: {'gradient_norm': 0.26535913348197937, '_timestamp': 1721140528.765423}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 5 is less than current step: 31. Dropping entry: {'gradient_norm': 0.25806814432144165, '_timestamp': 1721140535.4600892}).
[34m[1mwandb[39m[22m: [33mWARNING[39m (User provided step: 6 is less than current step: 31. Dropping entry: {'gradient_norm': 0.2588936686515808, '_timestamp': 1721140542.2171497}).
mse and l0 loss: 84577.0234375
R_loss: 0.9253959655761719
Average R_score: 0.07460402697324753
gradient_norm: 0.2588936686515808
mse and l0 loss: 82859.0859375
R_loss: 0.9220244288444519
Average R_score: 0.0779755562543869
gradient_norm: 0.25863945484161377
Training interrupted. Saving checkpoint...
